\chapter{Evaluation}\label{chap:evaluation}

This section reflects the results of all the testing performed with the different programming languages and architectures / types of computer.

\begin{table}[ht]
  \centering
  \begin{tabular}{lcccc}
    \toprule
                            & C\++                  & Go        & Python        & PyPy \\
    \midrule
    Intel Xeon Gold 6326 x2 &  GCC 14.2.0           & go1.24.2  & Python 3.12.3 & PyPy 7.3.19 \\
    MacbookPro M4 Pro       &  Apple Clang 17.0.0   & go1.24.2  & Python 3.13.3 & PyPy 7.3.19 \\
    RPi 5                   &                       &           &               &             \\
    Ryzen 3800x             &                       &           &               &             \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of language performance on different platforms}
  \label{tab:lang-platforms}
\end{table}

It should be noted that python should not be used for performance critical applications, as it is an interpreted language, and it is not designed for high performance computing. However, it is a great language for rapid prototyping and development, and it is widely used in the industry. 

Go's intent is to be a fast, efficient, and easy to use language, and it is designed for multithreading and concurrency, which makes it a great choice for high performance computing, specifically being designed for backend development for web applications, and it is widely used in the industry.

\section{Measurement Platforms}
\subsection{Many Core Platform}

This platform is the most powerful combinations as well as power-hungry combinations of all of my suite of devices. This is a rack server, with two processors Intel Xeon Gold 6326, that have each 16 cores, 32 threads, contributing to a total of 32 cores, 64 threads. It also has the largest amount of \gls{ram} from this testing, with 256GB of \gls{DDR4} memory.

As it has two sockets, one per CPU chip, there has to be an intercommunication between these processors if a process spreads out to more than 32 threads, or is set by the user using the command \texttt{taskset}, fixing the cores the process can run on.



%% Comments on the platform, ie two xeon 16 cores processors with hyperthreading, x86
\subsubsection{Evaluated parameters}
This system was the most versatile in terms of how many  tests could be done, as it has many processors, and uses Linux on x86, a great advantage to force processes to run on specific cores

The tests were done on a variety of core configuration, always setting, for core numbers less than 16, cores in the same processor. 

\begin{itemize}
    \item \textbf{1 Core}: Testing with one core, producing the baseline for the programs energy consumption and time.
    \item \textbf{2 Cores}: Testing with 2 cores provides the first glimpse of parallelization.
    \item \textbf{4 Cores}: Testing with 4 cores because many computer from some time ago had for cores.
    \item \textbf{8 Cores}: Testing with 8 cores gives us a great insight into how many processors in the market work, and it is half of the amount of cores inside one chip.
    \item \textbf{14 Cores}: Testing with 14 cores, because it is the number of cores available on the Laptop and wanted to have an execution time comparison. 
    \item \textbf{16 Cores}: Testing with 16 cores as it is the amount or real cores on a single chip. This must be one of the most energy efficient and fast tests, if there were only one \gls{cpu}.
    \item \textbf{28 Cores (in different \glspl{cpu}, but all real cores, no logical cores)}: Testing with 28 cores, distributed with two sockets is interesting because there has to be some information sharing over some bun inside the motherboard to synchronize both \glspl{cpu}. This wont be as energy efficient, but may be faster.
    \item \textbf{28 Cores (in the same CPU, 16 cores, 32 virtual cores)}: Testing with 28 cores, inside the same CPU, the performance should be slower as there are less real cores to tackle the work, but it has the advantage of not needing to share data to another socket.
    \item \textbf{32 Cores (same socket)}: Testing with 32 cores in the same socket is using all available logical threads of a system, the 16 real cores and the other 16 threads the \gls{cpu} has thanks to Hyper-Threading.
    \item \textbf{32 Cores (only real cores)}: Testing with 32 real cores, across two sockets should be the most powerful combination for CPU intensive tasks, as all the operations should be able to be carried out without many interruptions.
    \item \textbf{48 Cores}: Testing with 48 cores forces us to use all real cores and some logical cores.
    \item \textbf{60 Cores}: Testing with 60 cores is also interesting and not 64, as this would force the machine to interrupt the program we are benchmarking to perform routine operations, such as checking for incoming connections, or logging.
\end{itemize}

\subsubsection{Results}

The results for the server are shown in the following figures and tables. The energy consumption is measured in joules, and the execution time is measured in seconds. 

\input{parts/results/server-energy-pkg}

\input{parts/results/server-energy-ram}

\input{parts/results/server-execution-time}


From \autoref{fig:server-energy-pkg} we can see that the energy consumption of the server is not linear with the number of cores. It can be observed that the energy consumption decreases as the number of cores increases, but there is a point in the graph and \autoref{tab:server-energy-pkg} where the energy consumption starts to increase slightly again, as well as the execution times in \autoref{fig:server-execution-time}, but not as much as the energy consumption. 
This is due to hyperthreading \footnote{Hyperthreading is enabled in this system as it it not mine and I can not dissable it to perform testing. To set the process to a fixed \gls{cpu}, I used \texttt{taskset -c [cores]} ie \texttt{taskset -c 0-15,32-47} for running across multiple \glspl{cpu} and \texttt{taskset -c 0-31} to force the prorgam to only run in a single \gls{cpu}} in the \glspl{cpu}, which allows the \glspl{cpu} to run two threads per core, but this is not as efficient as running a single thread per core, as the \glspl{cpu} have to share resources between the two threads.

It is obvious from the multiple graphs and tables that the C\++ implementation is the most energy efficient and fastest by a significant margin, followed suprsingly by the PyPy execution of the Python code, which is faster than the Go implementation, and the Python implementation is the slowest and most energy consuming by an extremely large amount.

But, when loking close at the 28 core and 32 core tests, focusing in C++, we can see the energy consumption is lower when using cores from different CPUs rather than consuming more, as there is some energy efficiency loss when synchronizing the data between the two CPUs. But what happens in this case is that the C++ parallelization algorithm makes each of the cores have a very hard on \gls{cpu} workload, resulting in a more efficient result. This results in agreement with \autoref{sec:hyperthreading}, where it is explained that hyperthreading is not as efficient for specific tasks.



I want to specifically talk about the 60 cores test, as it is the most interesting one. In this test, the energy consumption is lower than in the 48 cores test, as well as the execution time on the C++ implementation, but on the Go implementation, both energy consumption and execution time are higher than in the 48 cores test. This is because the Go implementation is not as efficient as the C\++ implementation, and the Go runtime has to manage more goroutines, which adds overhead. 

Considering the 32 and 48 cores tests with the python program, the energy consumption reduces significanly when the program start using virtual cores, as the program is able to run on more cores, and the python runtime is not very demanding, being able to use these cores efficiently, as shown in \autoref{fig:server-energy-pkg} and \autoref{fig:server-execution-time} is an advantage to python with respect to itself.





\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/htop_not_running_100_60_cores.png}
    \caption{\gls{htop} showing the cores not being used at 100\% when using many cores for processing in a per-pixel multi threading renderer }
    \label{fig:htop_60_not_100}
\end{figure}

It also must be noted that the cores during the 48 core benchmark were being used at 100\% of their capacity, while in the 60 cores test, the cores were mostly being usead at a lower percentage, as shown in \autoref{fig:htop_60_not_100}. This is because the Go runtime is not able to efficiently use all the cores when there are more than 48 cores available, and it is not able to schedule the goroutines efficiently as these routines finish so fast that the Go runtime is not able to keep all the cores busy.

If we changed the implementation to a per-row renderer, on the go-side, the Go runtime would be able to use all the cores more efficiently, as it would be able to schedule the goroutines more efficiently, and the execution time would be lower, but the energy consumption would be higher, as the cores would be used at 100\% of their capacity. Thus, in this case, as we will see in other sections, having a faster execution time is not always the best option in terms of energy consumption.


% When changin to a per-pixel rendered, lower cores energy efficiency increased, but the many cores, from 48 oward would not get used as much, thus reducing the energy efficiency and increating the execution time. 



\subsection{Personal Desktop}
%% Comments on the platform x86, 8 cores, 16 threads, AMR ZEN 2 (https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-3000-series/amd-ryzen-7-3800x.html#amd_support_product_spec)
%% Test with and without hyerthreading as math intensive calculations
\subsubsection{Evaluated parameters}
\subsubsection{Results}

\subsection{Personal SOTA Laptop}
This lapot is said to have one of the fastest single-core performance in the market. It has a 14 core ARM processor, using the big.LITTLE architecture, with 10 high performance cores and 4 high efficiency cores. It has 48GB of RAM, which is enough to run any of the tests.


\subsubsection{Evaluated parameters}

This platform is a personal laptop, with a 14 core processor, the Apple M4 Pro, which has 10 high performance cores and 4 high efficiency cores, which is a big.LITTLE architecture. This means that the high performance cores are used for CPU intensive tasks, while the high efficiency cores are used for less demanding tasks, such as web browsing or watching videos. But as Apple does not allow the user seting the cores to be used by a specific process, like it happens in Linux, we can not test the high efficiency cores isolated from the high performance cores, as the operating system will decide for us which cores to use for each process. 



\subsubsection{Results}

\input{parts/results/mbp-energy}

\input{parts/results/mpb-execution-time.tex}

\subsection{Raspberry Pi 5}
\subsubsection{Evaluated parameters}
\subsubsection{Results}


\section{Comment on paralellizing different languages}

In this section, I would like to make some comment on the paralelization on different languages, and why some might experience a different behaviour. 

\subsection{Go}
When choosing how many "cores" the tests are using, for the Go implementation, I used the size of the \texttt{waitChan} channel. This number can be changed to be more than the total number of threads in the system, which sometimes increases the performance.


\begin{table}[ht]
  \centering
  \begin{tabular}{lcccc}
    \toprule
    Cores & Goroutines & Energy consumed (pkg) & Execution time \\
    \midrule
    1             & 1          &  28,522.69     &   172.95  \\ 
    1             & 2          &  28,919.31     &   175.38  \\ 
    2             & 2          &  18,231.97     &   89.319  \\
    2             & 4          &  18,224.50     &   89.275  \\
    4             & 4          &  10,304.27     &   45.508  \\
    4             & 8          &  10,299.06     &   45.482  \\
    8             & 8          &  5,617.27      &   23.781  \\
    8             & 16         &  5,580.52      &   23.594  \\
    14            & 14         &  3,155.30      &   14.034  \\
    14            & 28         &  3,151.93      &   14.001  \\
    16            & 16         &  2,904.52      &   12.456  \\
    16            & 32         &  3,018.54      &   12.435  \\
    28            & 28         &  2,306.35      &   8.9061  \\
    28 Same CPU   & 28         &  2,271.71      &   7.6137  \\
    28            & 56         &  2,314.29      &   7.7914  \\
    28 Same CPU   & 56         &  2,290.85      &   8.8153  \\
    32            & 32         &  2,151.74      &   8.1632  \\
    32 Same CPU   & 32         &  2,109.37      &   6.8224  \\
    32            & 64         &  2,121.88      &   8.1017  \\
    32 Same CPU   & 64         &  2,142.21      &   6.8960  \\
    48            & 48         &  1,856.93      &   5.7187  \\
    48            & 96         &  1,848.47      &   5.6737  \\
    60            & 60         &  1,744.76      &   5.3571  \\
    60            & 120        &  1,737.80      &   5.3208  \\
    60            & 200        &  1,724.73      &   5.2556  \\
    60            & 250        &  1,719.49      &   5.2182  \\
    \bottomrule
  \end{tabular}
  \caption{Go goroutines and threads used in the tests}
  \label{tab:go-routines-cores}
\end{table}

As it can be seen from \autoref{tab:go-routines-cores}, the Go implementation is able to use more than the total number of threads in the system, and it is able to use them efficiently, as the Go runtime is able to schedule the goroutines efficiently.

\subsection{Python}
When iterating though every pixel in Python, as the environment has to be copied for every single pixel, the cores are not being used at 100\% of their capacity, specifically, while testing I saw that the cores were being used at around 5\% of their capacity. Meaning the creation of too many threads is not beneficial, as the overhead of creating the threads is larger than the actual work being done by each thread. 
Another factor that Python, each time a task is submitted to a process, Python needs to serialize (pickle) the entire world object and other parameters, then deserialize them in the worker process, which means that, if this has to happen for every pixel, the serializing and deselializing tasks run for much longer than the actual pixel processing.

